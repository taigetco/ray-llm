apiVersion: ray.io/v1
kind: RayService
metadata:
  name: rayllm
spec:
  serviceUnhealthySecondThreshold: 1200 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 1200 # Config for the health check threshold for deployments. Default value is 60.
  serveConfigV2: |
      applications:
      - name: llm-router
        import_path: rayllm.backend:router_application
        route_prefix: /
        runtime_env:
          working_dir: "https://raw.githubusercontent.com/taigetco/ray-llm/master/rayllm.zip"
        args:
          models:
            - /model-yaml/mistralai--Mixtral-7b-Instruct-v02.yaml
  rayClusterConfig:
    rayVersion: '2.7.0'
    # Ray head pod template
    headGroupSpec:
      # The `rayStartParams` are used to configure the `ray start` command.
      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
      rayStartParams:
        resources: '"{\"accelerator_type_cpu\": 2}"'
        dashboard-host: '0.0.0.0'
      #pod template
      template:
        spec:
          containers:
          - name: ray-head
            image: anyscale/ray-llm:latest
            resources:
              limits:
                cpu: 2
                memory: 8Gi
              requests:
                cpu: 2
                memory: 8Gi
            volumeMounts:
              - mountPath: /data/ray/model
                name: model
              - mountPath: /model-yaml
                name: model-yaml
            ports:
            - containerPort: 6379
              name: llm-server
            - containerPort: 8265 # Ray dashboard
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
          volumes:
            - name: model
              hostPath:
                path: "/data/ray/model"
                type: Directory
            - name: model-yaml
              hostPath:
                path: "/data/ray/ray-llm/models/continuous_batching"
                type: Directory
    workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 1
      minReplicas: 0
      maxReplicas: 1
      # logical group name, for this called small-group, also can be functional
      groupName: gpu-group
      rayStartParams:
        resources: '"{\"accelerator_type_cpu\": 15, \"accelerator_type_a10\": 2}"'
      # Pod template
      template:
        spec:
          containers:
          - name: llm
            image: anyscale/ray-llm:latest
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            resources:
              limits:
                cpu: "15"
                memory: "20G"
                nvidia.com/gpu: 2
              requests:
                cpu: "15"
                memory: "20G"
                nvidia.com/gpu: 2
            ports:
            - containerPort: 8000
              name: serve
          # Please add the following taints to the GPU node.
          tolerations:
            - key: "ray.io/node-type"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
